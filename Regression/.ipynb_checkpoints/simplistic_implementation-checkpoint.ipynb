{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict \n",
    "from torch.nn import init\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3158,  0.0069],\n",
      "        [-0.4726, -0.3946],\n",
      "        [-1.6537, -0.8888],\n",
      "        [ 0.7861, -1.0481],\n",
      "        [-0.1916,  1.4963],\n",
      "        [ 1.7995, -0.7892],\n",
      "        [-0.7381, -0.4959],\n",
      "        [ 0.5652, -0.4595],\n",
      "        [-1.4829, -0.8312],\n",
      "        [-0.1050,  0.2419]]) tensor([ 4.8145,  4.5867,  3.9143,  9.3281, -1.2744, 10.4839,  4.4225,  6.8925,\n",
      "         4.0687,  3.1628])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break\n",
    "# test1 = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[1,2,0]])\n",
    "# test2 = torch.tensor([[5,6],[1,10],[8,9]])\n",
    "# testset = Data.TensorDataset(test1, test2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "net = LinearNet(num_inputs)\n",
    "# print(net)\n",
    "# class A():\n",
    "#     def __init__(self,num):\n",
    "#         #super(C, self).__init__()\n",
    "#         print('我說',num)\n",
    "#         self.num = num\n",
    "        \n",
    "# class B(A):\n",
    "#     def __init__(self,num):\n",
    "#         print('B init')\n",
    "#         super(B, self).__init__(num)   \n",
    "# b = B(9)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0118, 0.0104], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#recreate the nn\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    #add other layers\n",
    ")\n",
    "\n",
    "#re-recreate the nn\n",
    "# net = nn.Sequential()\n",
    "# net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "\n",
    "#re-re-recreate the nn with a OrderedDict \n",
    "# net = nn.Sequential(OrderedDict([\n",
    "#     ('linear', nn.Linear(num_inputs, 1, bias=True)) #default is True\n",
    "# ]))\n",
    "\n",
    "#assign parameters manually \n",
    "net[0].weight = nn.Parameter(torch.tensor([2.0, 5.0]))\n",
    "net[0].bias = nn.Parameter(torch.tensor([666.0]))\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    #add other layers\n",
    ")\n",
    "#re-assign parameters with init\n",
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0)\n",
    "\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instantiate the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "# print(optimizer)\n",
    "# for param in net:\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure different learning rates for subnets which has nothing to\n",
    "# do with us right now\n",
    "# optimizer =optim.SGD([\n",
    "#                 # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "#                 {'params': net.subnet1.parameters()}, # lr=0.03\n",
    "#                 {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "#             ], lr=0.03)\n",
    "\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] *= 0.1 #set learning to 0.1 of before\n",
    "#     pp.pprint(param_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000961\n",
      "epoch 2, loss 0.000077\n",
      "epoch 3, loss 0.000073\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        #print(X[0], net[0].weight)\n",
    "        output = net(X)\n",
    "        l = loss(output.view(y.size()), y) #reshape the target tensor with view(), \n",
    "                                        #first dimension -1 is inferred from the other dimension, \n",
    "                                        #second dimension is set to 1\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] Parameter containing:\n",
      "tensor([[ 2.0000, -3.4003]], requires_grad=True)\n",
      "4.2 Parameter containing:\n",
      "tensor([4.1995], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w, net[0].weight)\n",
    "print(true_b, net[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
